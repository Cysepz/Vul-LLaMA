import os
import subprocess
from pathlib import Path
from transformers import AutoTokenizer

import logging

def setup_logger(log_path):
    """è¨­å®š loggerï¼Œå°‡è¼¸å‡ºå¯«å…¥æŒ‡å®š txt æª”æ¡ˆ"""
    logger = logging.getLogger("StatLogger")
    logger.setLevel(logging.INFO)

    # æ¸…ç©ºèˆŠ handlerï¼ˆé¿å…é‡è¤‡å¯«å…¥ï¼‰
    if logger.hasHandlers():
        logger.handlers.clear()

    # æª”æ¡ˆè¼¸å‡º
    file_handler = logging.FileHandler(log_path, mode='w', encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(file_handler)

    # çµ‚ç«¯è¼¸å‡º
    stream_handler = logging.StreamHandler()
    stream_handler.setFormatter(logging.Formatter('%(message)s'))
    logger.addHandler(stream_handler)

    return logger

def clone_aosp_modules(aosp_dir, logger):
    """ä¸‹è¼‰ AOSP ä¸­å¸¸ç”¨æ¨¡çµ„çš„ Java åŸå§‹ç¢¼"""
    os.makedirs(aosp_dir, exist_ok=True)
    repos = [
        "https://android.googlesource.com/platform/frameworks/base",
    ]

    for repo in repos:
        repo_name = repo.split("/")[-1]
        dest_path = os.path.join(aosp_dir, repo_name)
        if not os.path.exists(dest_path):
            logger.info(f"ğŸ“¥ Cloning {repo} ...")
            subprocess.run(["git", "clone", "--depth", "1", repo, dest_path])
        else:
            logger.info(f"âœ… {dest_path} already exists, skipping.")
    
    return dest_path

def extract_codellama_tokenizer_vocab(tokenizer_name: str, output_path: str):
    """å¾åŸå§‹ CodeLLaMA tokenizer æŠ½å‡º vocab token ä¸¦å¯«æˆæ–‡å­—æª”æ¨¡æ“¬èªæ–™"""
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    vocab_tokens = list(tokenizer.get_vocab().keys())

    with open(output_path, "w", encoding="utf-8") as f:
        for token in vocab_tokens:
            if token.startswith("â–"):
                f.write(token.replace("â–", "") + "\n")

    print(f"âœ… åŸå§‹ vocab token èªæ–™å·²å„²å­˜è‡³ {output_path}")


def collect_java_kotlin_files(root_dir, extensions=(".java", ".kt")):
    """æ”¶é›†æŒ‡å®šå‰¯æª”åçš„ç¨‹å¼ç¢¼æª”æ¡ˆ"""
    files = []
    for path in Path(root_dir).rglob("*"):
        if path.suffix in extensions:
            files.append(str(path))
    return files


def build_aosp_corpus(file_list, output_path: str, logger=None):
    """å°‡ AOSP ç¨‹å¼ç¢¼æ•´ç†æˆè¨“ç·´èªæ–™ï¼ˆåŒ…å«æ›è¡Œèˆ‡ç¸®æ’ï¼‰"""
    with open(output_path, "w", encoding="utf-8") as out_f:
        for file_path in file_list:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    out_f.write(f.read())
                    out_f.write("\n\n")
            except Exception as e:
                logger.warning(f"[Warning] Failed to read {file_path}: {e}")
    logger.info(f"âœ… AOSP corpus å„²å­˜è‡³ {output_path}")


def merge_corpora(vocab_path: str, aosp_path: str, output_path: str, logger):
    """åˆä½µ vocab èªæ–™èˆ‡ AOSP èªæ–™"""
    with open(output_path, "w", encoding="utf-8") as out_f:
        for path in [vocab_path, aosp_path]:
            with open(path, "r", encoding="utf-8") as f:
                out_f.write(f.read())
                out_f.write("\n")
    logger.info(f"âœ… åˆä½µèªæ–™å„²å­˜è‡³ {output_path}")


def train_tokenizer_from_corpus(corpus_path: str, base_tokenizer: str, save_dir: str, vocab_size: int = 32000, logger=None):
    """ç”¨åˆä½µå¾Œèªæ–™é‡æ–°è¨“ç·´ tokenizerï¼Œä¿ç•™ CodeLLaMA çš„è¡Œç‚º"""
    tokenizer = AutoTokenizer.from_pretrained(base_tokenizer, use_fase=True)

    def get_lines(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    yield line

    new_tokenizer = tokenizer.train_new_from_iterator(
        get_lines(corpus_path),
        vocab_size=vocab_size
    )
    new_tokenizer.save_pretrained(save_dir)
    logger.info(f"âœ… æ–° tokenizer å·²å„²å­˜è‡³ {save_dir}")

import os
import sentencepiece as spm
from transformers import LlamaTokenizer

def train_sentencepiece_tokenizer(corpus_path, save_dir, vocab_size=32000, logger=None):
    """ä½¿ç”¨ SentencePiece è¨“ç·´ tokenizerï¼Œä¿ç•™ byte fallback èˆ‡ Llama ç›¸å®¹"""
    os.makedirs(save_dir, exist_ok=True)
    model_prefix = os.path.join(save_dir, "spm")

    logger.info("ğŸš€ é–‹å§‹è¨“ç·´ SentencePiece tokenizer ...")
    spm.SentencePieceTrainer.train(
        input=corpus_path,
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type="bpe",
        character_coverage=1.0,
        byte_fallback=True,            # âœ… ä¿ç•™ <0x0A> ç­‰ç‰¹æ®Šç¬¦è™Ÿ
        train_extremely_large_corpus=True,
        pad_id=0, pad_piece="[PAD]",
        unk_id=1, unk_piece="[UNK]",
        bos_id=2, bos_piece="[BOS]",
        eos_id=3, eos_piece="[EOS]"
    )
    logger.info(f"âœ… SentencePiece æ¨¡å‹å„²å­˜æ–¼ï¼š{model_prefix}.model")

    logger.info("ğŸ“¦ åŒ¯å‡ºç‚º HuggingFace æ ¼å¼ tokenizer")
    tokenizer = LlamaTokenizer(vocab_file=f"{model_prefix}.model")
    tokenizer.save_pretrained(save_dir)
    logger.info(f"âœ… HuggingFace tokenizer å„²å­˜è‡³ï¼š{save_dir}/")



def test(tokenizer_path: str, logger):
    """æ¸¬è©¦æ–°è¨“ç·´å¥½çš„ tokenizer åˆ†è©è¡Œç‚ºï¼Œèˆ‡åŸå§‹ CodeLLaMA åˆ†è©å™¨å°ç…§"""

    logger.info("ğŸ” æ¸¬è©¦æ–° tokenizer åˆ†è©æ•ˆæœï¼ˆèˆ‡åŸå§‹ tokenizer æ¯”è¼ƒï¼‰ï¼š")

    test_cases = [
        # ä¸€èˆ¬ Java code
        "public static void main(String[] args) { System.out.println(\"Hello World\"); }",
        "Intent intent = new Intent(this, TargetActivity.class);",
        "if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.M) { requestPermissions(); }",

        # 4. æ›è¡Œæ¸¬è©¦
        "String a = \"line1\";\nString b = \"line2\";",

        # 5. Tab æ¸¬è©¦ï¼ˆæ³¨æ„ï¼šé€™æ˜¯ tabï¼Œä¸æ˜¯ 4 å€‹ç©ºæ ¼ï¼‰
        "\tif (x == 1) {\n\t\tdoSomething();\n\t}",

        # 6. å¤šç©ºç™½æ¸¬è©¦ï¼ˆå››å€‹ spaceï¼‰
        "    return a + b;",

        # 7. AOSP-style function
        "private void checkPermission() {\n    if (ContextCompat.checkSelfPermission(this, Manifest.permission.READ_CONTACTS) != PackageManager.PERMISSION_GRANTED) {\n        // ask again\n    }\n}",
    
        # å…¶ä»– Android å¸¸é‡æ¸¬è©¦
        # 8. Android permission å¸¸é‡
        "if (ContextCompat.checkSelfPermission(this, Manifest.permission.ACCESS_FINE_LOCATION) != PackageManager.PERMISSION_GRANTED) { return; }",

        # 9. Android settings
        "Intent intent = new Intent(Settings.ACTION_MANAGE_OVERLAY_PERMISSION);",

        # 10. TelephonyManager æ–¹æ³•
        "String deviceId = telephonyManager.getDeviceId();",

        # 11. SharedPreferences ç·¨è¼¯å™¨
        "SharedPreferences.Editor editor = prefs.edit(); editor.putString(\"key\", \"value\").apply();",

        # 12. å–å¾—è³‡æºé¡è‰²
        "int white = ContextCompat.getColor(context, R.color.white);",

        # 13. NotificationChannel å»ºç«‹
        "NotificationChannel channel = new NotificationChannel(CHANNEL_ID, \"Channel name\", NotificationManager.IMPORTANCE_HIGH);",

        # 14. ä½¿ç”¨ ContentResolver æŸ¥è©¢è¯çµ¡äºº
        "Cursor cursor = getContentResolver().query(ContactsContract.Contacts.CONTENT_URI, null, null, null, null);",
    ]

    original_tokenizer = AutoTokenizer.from_pretrained("codellama/CodeLlama-7b-hf", use_fast=True)
    new_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True)
    
    vocab = new_tokenizer.get_vocab()
    # æª¢æŸ¥æ˜¯å¦åŒ…å«å¸¸è¦‹ fallback byte
    for b in ["<0x0A>", "<0x09>", "<0x20>"]:
        print(f"{b} in vocab? {'Yes' if b in vocab else 'No'}")


    for idx, code in enumerate(test_cases):
        logger.info(f"\nã€Test Case {idx+1}ã€‘åŸå§‹ç¢¼ï¼š\n{code}\n")

        orig_tokens = original_tokenizer.tokenize(code)
        new_tokens = new_tokenizer.tokenize(code)

        logger.info(f"ğŸ§  åŸå§‹ tokenizer åˆ†è©ï¼š\n{orig_tokens}\n")
        logger.info(f"ğŸ†• æ–° tokenizer åˆ†è©ï¼š\n{new_tokens}\n")

        if orig_tokens == new_tokens:
            logger.info("âœ… åˆ†è©å®Œå…¨ä¸€è‡´\n")
        else:
            logger.info("â— åˆ†è©çµæœä¸åŒ\n")



def main():
    log_path = "./vul_llama/train_logs/train_bpe_tokenizer.log"

    logger = setup_logger(log_path)

    BASE_TOKENIZER = "codellama/CodeLlama-7b-hf"
    AOSP_ROOT = "./vul_llama/tokenizer/AOSP"
    AOSP_DIR = clone_aosp_modules(AOSP_ROOT, logger)
    SAVE_DIR = "./vul_llama/android_tokenizer"

    VOCAB_CORPUS = "./vul_llama/tokenizer/codellama_vocab.txt"
    AOSP_CORPUS = "./vul_llama/tokenizer/aosp_corpus.txt"
    MERGED_CORPUS = "./vul_llama/tokenizer/merged_corpus.txt"

    extract_codellama_tokenizer_vocab(BASE_TOKENIZER, VOCAB_CORPUS)    
    code_files = collect_java_kotlin_files(AOSP_DIR)
    logger.info(f"å…±æ‰¾åˆ° {len(code_files)} å€‹ AOSP æª”æ¡ˆ")
    build_aosp_corpus(code_files, AOSP_CORPUS, logger=logger)
    merge_corpora(VOCAB_CORPUS, AOSP_CORPUS, MERGED_CORPUS, logger=logger)
    train_tokenizer_from_corpus(MERGED_CORPUS, BASE_TOKENIZER, SAVE_DIR, vocab_size=32000, logger=logger)
    SAVE_DIR = "./vul_llama/android_tokenizer_spm"
    train_sentencepiece_tokenizer(MERGED_CORPUS, SAVE_DIR, vocab_size=32000, logger=logger)

    test(SAVE_DIR, logger)


if __name__ == "__main__":
    main()